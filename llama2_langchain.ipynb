{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install langchain"
      ],
      "metadata": {
        "id": "TOyVaq6r3oVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ce55b3-3c2d-4fea-d97a-b76db47b7642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "61e3daeb-2c38-4021-e2d8-8fd44d8e6872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  1 03:19:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "af6f1e22601e4a68b1a631d3d19f24d3",
            "8a351400706d4da4a7bf5465c902eb67",
            "7a6f269ee05a494f8ea9e2562586ec11",
            "22ab218d9c294411bd2f118ae8e12eca",
            "84a99924b765437899c7cdf9c0c9ddc8",
            "198732fb3afa428fa4f481dd8fca0f31",
            "fc7c1c01f4df4809810d48ac6ebdeb7c",
            "de951e9d5c7d4c04b34ec25a1c51b225",
            "5265e9a9f2094cb7b56a74cfc91003c0",
            "5f104238bcf448b299913272e524956c",
            "681817af6438438f8882afebc5ba0d0f",
            "b7d18a7731904372acee05dfab680094",
            "680715e056614a588c6dd9496261ad30",
            "565eac4c4a514d2c82e620b0868a337f",
            "5a913a833b954a2898bbadb11324e75d",
            "e7c20ff1514e4da0ad29dfda190d2763",
            "d2f656b98c4a4ff9957a0f2d11fe4dbd",
            "d54b469234f14ab59722298922185ef7",
            "7846cfb18da54bb6a944f41f8fe97e3f",
            "c96828febb8149aea0510948c6c62085",
            "eadcd287fe554bc897ece84471179ce8",
            "b613ddcd4cbb4f18b1fb3b743d082fc9",
            "5fce373fbe5f429ea6f443851bbead0c",
            "c4610ae883b345869960eec412d7a36a",
            "7eaa651e4189445eb6ed30147f12e33b",
            "92b6adec33f946ab9c90bb460b8f1146",
            "202c474b79c44a229cf2722ee3d46de7",
            "d0a3a2ecb5d944a7b771e15042dab818",
            "19360207a71d4254981d6f826c8f6e95",
            "33685612875f4bf1ae2688cdbfb93cd3",
            "15c3cccdb21c423e84094ea443eb6c5f",
            "0ff24ed5b6d04c30b512a7fdab3fa9c2"
          ]
        },
        "id": "NItbG3Ooedqs",
        "outputId": "91d590a2-cd71-4acb-b338-0ebb94bbd175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af6f1e22601e4a68b1a631d3d19f24d3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA2 7B Chat\n"
      ],
      "metadata": {
        "id": "H0shki19igLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             load_in_8bit=True,\n",
        "                                            #  load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "68b54ff2930747d69f2185e404e147cd",
            "836405390036405db91880afd10b32a1",
            "16e1e56c8ed24a2c880bd08aef948420",
            "97deadf494454f0c8c7c246d79320036",
            "72ea6d33efa349d8902e7abc1e80313d",
            "0ec9fd7961d34476a70beb35579d6d94",
            "0f52f35bdbec4ee4b22d72b84a858b20",
            "d73d1782f8d143dc9c8fdb2d8d259fea",
            "51c5e7e09c894d6eadefb901a9f73487",
            "494fb611c431435cb0d0c3fdaaaca159",
            "bf3e7407fd654fc39b2fc6ba5d8f754a"
          ]
        },
        "id": "B4Xq1at9iixB",
        "outputId": "6343c2ef-8e78-44a8-87a6-a0ff885f5b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68b54ff2930747d69f2185e404e147cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline for later\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ],
      "metadata": {
        "id": "fvuuA6wF1JH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8IS_kfwtE1uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1acb4b-591d-41d7-fdd4-19cf12cb61df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  1 03:21:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    28W /  70W |   7921MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo-FSysZiVkA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    prompt = get_prompt(text)\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_new_tokens=512,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs#, outputs\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"What is the temperature in Melbourne?\"\n",
        "\n",
        "get_prompt(instruction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oTf-vPB8wPGs",
        "outputId": "5ad58a3b-b5f4-42d6-81cc-bc514db1f02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nWhat is the temperature in Melbourne?[/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following text for me {text}\"\n",
        "\n",
        "system_prompt = \"You are an expert and summarization and reducing the number of words used\"\n",
        "\n",
        "get_prompt(instruction, system_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "R7n4grsO1UN1",
        "outputId": "4ad3b316-f993-40ef-fdf8-f4f9ad8ea776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[INST]<<SYS>>\\nYou are an expert and summarization and reducing the number of words used\\n<</SYS>>\\n\\nSummarize the following text for me {text}[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain basics"
      ],
      "metadata": {
        "id": "-fTKdQRCdis7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain"
      ],
      "metadata": {
        "id": "SP4Bk5YBf1mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are an advanced assistant that excels at translation. \"\n",
        "instruction = \"Convert the following text from English to French:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "dTBrt6XmuY3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d067481-25b7-4dd8-da43-cebc848c359d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an advanced assistant that excels at translation. \n",
            "<</SYS>>\n",
            "\n",
            "Convert the following text from English to French:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how are you today?\"\n",
        "output = llm_chain.run(text)\n",
        "\n",
        "parse_text(output)"
      ],
      "metadata": {
        "id": "B0fUGp-muY6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e8beef-6115-422c-ef8a-07eb135dc84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Bien sûr! Here is the translation of \"how are you today?\" in French:  Comment allez-vous\n",
            "aujourd'hui?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization"
      ],
      "metadata": {
        "id": "e38eSqFb3hET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following article for me {text}\"\n",
        "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "Dt4mn5CJuY90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d902baec-cddf-4bb9-eb7b-54c4ff75facb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an expert and summarization and expressing key ideas succintly\n",
            "<</SYS>>\n",
            "\n",
            "Summarize the following article for me {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(input_string):\n",
        "    words = input_string.split(\" \")\n",
        "    return len(words)\n",
        "\n",
        "text = '''Twitter (now X) CEO Linda Yaccarino claims usage at ‘all time high’ in memo to staff\n",
        "Twitter’s (now X’s) newly established CEO Linda Yaccarino touts the company’s success and X’s future plans in a company-wide memo obtained by CNBC. The exec once again claims, without sharing any specific metrics, that the service’s usage is at an “all time high,” and hints at what’s to come in terms of new product experiences for the newly rebranded platform.\n",
        "\n",
        "The service formerly known as Twitter has been working to become more than just a social network and more of an “everything app,” as owner Elon Musk dubbed it.\n",
        "\n",
        "As the Telsa and Space X exec explained in October 2022, telegraphing Twitter’s eventual rebranding, buying Twitter was meant to be “an accelerant to creating X, the everything app.”\n",
        "\n",
        "\n",
        "His grand plan has been to create an app that allows creators to monetize their content, then later moves into payments services and even banking, Musk remarked during a Twitter Spaces livestream with advertisers in November. At the time, he even mentioned the possibility of establishing money market accounts on Twitter that would pay a high-interest rate to attract consumers to X.\n",
        "\n",
        "Those possible product concepts were again referenced in Yaccarino’s new missive, when she writes, “Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.”\n",
        "\n",
        "Twitter, now X, has already implemented some of Musk’s ideas around videos and creator monetization. In May, the company began allowing subscribers to upload two-hour videos to its service, which advertiser Apple then leveraged when it released the entire first episode of its hit Apple TV+ show “Silo” on the platform. Fired Fox News host Tucker Carlson had been posting lengthy videos to Twitter as well, until ordered to stop by the network.\n",
        "\n",
        "In addition, earlier this month, Twitter began sharing ad revenue with verified creators.\n",
        "\n",
        "However, all is not well at Twitter X, whose traffic — at least by third-party measurements — has been dropping. Data from web analytics firm Similarweb indicated Twitter’s web traffic declined 5% for the first two days its latest rival, Instagram Threads, became generally available, compared with the week prior. Plus, Similarweb said Twitter’s web traffic was down 11% compared with the same days in 2022. Additionally, Cloudflare CEO Matthew Prince earlier this month tweeted a graph of traffic to the Twitter.com domain that showed “Twitter traffic tanking,” he said.\n",
        "\n",
        "\n",
        "Yaccarino subtly pushed back at those reports at the time, claiming that Twitter had its largest usage day since February in early July. She did not share any specific metrics or data. At the same time, however, the company was quietly blocking links to Threads.net in Twitter searches, suggesting it was concerned about the new competition.\n",
        "\n",
        "Today, Yaccarino repeats her vague claims around X’s high usage in her company-wide memo even as analysts at Forrester are predicting X will either shut down or be acquired within the next 12 months and numerous critics concur that the X rebrand is destined to fail.\n",
        "\n",
        "Yaccarino’s memo, otherwise, was mostly a lot of cheerleading, applauding X’s team for their work and touting X’s ability to “impress the world all over again,” as Twitter once did.\n",
        "\n",
        "The full memo, courtesy of CBNC, is below:\n",
        "\n",
        "Hi team,\n",
        "\n",
        "What a momentous weekend. As I said yesterday, it’s extremely rare, whether it’s in life or in business, that you have the opportunity to make another big impression. That’s what we’re experiencing together, in real time. Take a moment to put it all into perspective.\n",
        "\n",
        "17 years ago, Twitter made a lasting imprint on the world. The platform changed the speed at which people accessed information. It created a new dynamic for how people communicated, debated, and responded to things happening in the world. Twitter introduced a new way for people, public figures, and brands to build long lasting relationships. In one way or another, everyone here is a driving force in that change. But equally all our users and partners constantly challenged us to dream bigger, to innovate faster, and to fulfill our great potential.\n",
        "\n",
        "With X we will go even further to transform the global town square — and impress the world all over again.\n",
        "\n",
        "Our company uniquely has the drive to make this possible. Many companies say they want to move fast — but we enjoy moving at the speed of light, and when we do, that’s X. At our core, we have an inventor mindset — constantly learning, testing out new approaches, changing to get it right and ultimately succeeding.\n",
        "\n",
        "With X, we serve our entire community of users and customers by working tirelessly to preserve free expression and choice, create limitless interactivity, and create a marketplace that enables the economic success of all its participants.\n",
        "\n",
        "The best news is we’re well underway. Everyone should be proud of the pace of innovation over the last nine months — from long form content, to creator monetization, and tremendous advancements in brand safety protections. Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.\n",
        "\n",
        "Please don’t take this moment for granted. You’re writing history, and there’s no limit to our transformation. And everyone, is invited to build X with us.\n",
        "\n",
        "Elon and I will be working across every team and partner to bring X to the world. That includes keeping our entire community up to date, ensuring that we all have the information we need to move forward.\n",
        "\n",
        "Now, let’s go make that next big impression on the world, together.\n",
        "\n",
        "Linda'''\n",
        "\n",
        "count_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gykQOf4OuZBK",
        "outputId": "7f483d3a-dec7-45ff-9210-1eae2353e131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "940"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_chain.run(text)\n",
        "print(count_words(output))\n",
        "parse_text(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4VDYh3VuZD_",
        "outputId": "f1c859dd-af49-46ee-890f-7a606999ee96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131\n",
            "  Twitter (now X) CEO Linda Yaccarino has claimed in a company-wide memo that the platform's usage\n",
            "is at an \"all-time high,\" despite recent traffic declines. Yaccarino did not provide specific\n",
            "metrics to back up her claim, but touted the company's success and future plans, including expanding\n",
            "into new product experiences such as audio, video, messaging, payments, and banking. The memo also\n",
            "mentioned Elon Musk's vision for X as an \"everything app\" and the company's drive to preserve free\n",
            "expression and choice. However, third-party measurements have shown traffic declines for Twitter,\n",
            "and some analysts predict that the platform will either shut down or be acquired within the next 12\n",
            "months. Despite these challenges, Yaccarino remains optimistic about X's future and encouraged\n",
            "employees to continue innovating and pushing the platform forward.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chatbot"
      ],
      "metadata": {
        "id": "MsPFAOBK74Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate"
      ],
      "metadata": {
        "id": "Inghmejz_Tkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
        "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7DAJoLEAOU7",
        "outputId": "f0e94a26-443f-41a3-c03e-826cb4015ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "{chat_history} \n",
            "\n",
            "User: {user_input}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "Qv7iPmbI_TnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "CGjjiqxz_TqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Hi, my name is Sam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "X3VadjbM_Tsx",
        "outputId": "2c0d1896-1e84-477d-de48-6ac80acf7595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            " \n",
            "\n",
            "User: Hi, my name is Sam[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Hello Sam! How may I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Can you tell me about yourself.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ENqnCnun_TvC",
        "outputId": "4c7f199d-bf69-4981-afed-7d2469ded981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today? \n",
            "\n",
            "User: Can you tell me about yourself.[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Today is Friday. What number day of the week is that?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "_HIqTiLc_Tx2",
        "outputId": "632dd6bf-de1e-42b3-e8e4-7e0ba0d96e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for? \n",
            "\n",
            "User: Today is Friday. What number day of the week is that?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  AI: Ah, a great question! Friday is the 5th day of the week.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"what is the day today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "r5Kn6p15Ky2F",
        "outputId": "a0b1c489-479b-4895-e551-c95349fbdbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! It's nice to meet you. How can I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Great question! Friday is the 5th day of the week. \n",
            "\n",
            "User: what is the day today?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  AI: Today is Friday! 😊'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "CGXX3RoT74qz",
        "outputId": "593200b1-736f-4e77-b2e4-876ddaf99e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week. \n",
            "\n",
            "User: What is my name?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Can you tell me about the olympics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "YCBnZHm9BOQU",
        "outputId": "ef5313f9-a3fa-42a8-ce7d-dcafc1f03068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for? \n",
            "\n",
            "User: Can you tell me about the olympics[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\\n\\nHow may I assist you further?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"What have we talked about in this Chat?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Nf2QRotKBOTq",
        "outputId": "cafb99e4-6055-47aa-b45f-6845e9bc125b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Can you tell me about the olympics\n",
            "AI:   Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\n",
            "\n",
            "How may I assist you further? \n",
            "\n",
            "User: What have we talked about in this Chat?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Of course! Based on the chat history you provided, here are the topics we discussed:\\n\\n1. Introduction: The assistant greeted Sam and asked how they may assist them.\\n2. Personal information: Sam asked the assistant to tell them about themselves, and the assistant provided some basic information about being an AI assistant.\\n3. Days of the week: Sam asked the assistant what day of the week it was, and the assistant replied that it was Friday, which is the 5th day of the week.\\n4. Name: Sam asked the assistant to tell them their name, and the assistant politely declined and asked if there was anything else they could help with.\\n5. Olympics: Sam asked the assistant to tell them about the Olympics, and the assistant provided some general information about the Olympics, including that they are a series of international sporting events that take place every four years and are organized by the International Olympic Committee (IOC).\\n\\nI hope that helps! Let me know if you have any other questions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"What is the Retrieval Augmetation Generation?\")"
      ],
      "metadata": {
        "id": "W4aLRBjEBOW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "2df6a905-2ec1-444a-f924-6446009cdf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Can you tell me about the olympics\n",
            "AI:   Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\n",
            "\n",
            "How may I assist you further?\n",
            "Human: What have we talked about in this Chat?\n",
            "AI:   Of course! Based on the chat history you provided, here are the topics we discussed:\n",
            "\n",
            "1. Introduction: The assistant greeted Sam and asked how they may assist them.\n",
            "2. Personal information: Sam asked the assistant to tell them about themselves, and the assistant provided some basic information about being an AI assistant.\n",
            "3. Days of the week: Sam asked the assistant what day of the week it was, and the assistant replied that it was Friday, which is the 5th day of the week.\n",
            "4. Name: Sam asked the assistant to tell them their name, and the assistant politely declined and asked if there was anything else they could help with.\n",
            "5. Olympics: Sam asked the assistant to tell them about the Olympics, and the assistant provided some general information about the Olympics, including that they are a series of international sporting events that take place every four years and are organized by the International Olympic Committee (IOC).\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions. \n",
            "\n",
            "User: What is the Retrieval Augmetation Generation?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Hello! I'm here to help you with any questions or tasks you may have. The Retrieval Augmentation Generation (RAG) is a term used in the field of natural language processing (NLP) to describe a type of language model that is trained to generate text that is similar to a given prompt or input.\\n\\nRAG models are designed to improve the performance of language models by generating new text that is similar to the training data, but not necessarily identical to it. This can help the model to generate more diverse and creative responses to given prompts, and can also help to reduce the risk of overfitting to a particular dataset.\\n\\nI hope that helps! Let me know if you have any other questions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Explain RLHF in 300 words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "BK8nzgyZ7Xe3",
        "outputId": "bfd52384-51fb-4ade-f131-401059c79f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Can you tell me about the olympics\n",
            "AI:   Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\n",
            "\n",
            "How may I assist you further?\n",
            "Human: What have we talked about in this Chat?\n",
            "AI:   Of course! Based on the chat history you provided, here are the topics we discussed:\n",
            "\n",
            "1. Introduction: The assistant greeted Sam and asked how they may assist them.\n",
            "2. Personal information: Sam asked the assistant to tell them about themselves, and the assistant provided some basic information about being an AI assistant.\n",
            "3. Days of the week: Sam asked the assistant what day of the week it was, and the assistant replied that it was Friday, which is the 5th day of the week.\n",
            "4. Name: Sam asked the assistant to tell them their name, and the assistant politely declined and asked if there was anything else they could help with.\n",
            "5. Olympics: Sam asked the assistant to tell them about the Olympics, and the assistant provided some general information about the Olympics, including that they are a series of international sporting events that take place every four years and are organized by the International Olympic Committee (IOC).\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions.\n",
            "Human: What is the Retrieval Augmetation Generation?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. The Retrieval Augmentation Generation (RAG) is a term used in the field of natural language processing (NLP) to describe a type of language model that is trained to generate text that is similar to a given prompt or input.\n",
            "\n",
            "RAG models are designed to improve the performance of language models by generating new text that is similar to the training data, but not necessarily identical to it. This can help the model to generate more diverse and creative responses to given prompts, and can also help to reduce the risk of overfitting to a particular dataset.\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions. \n",
            "\n",
            "User: Explain RLHF in 300 words[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Of course! Reinforcement Learning with Human Feedback (RLHF) is a machine learning approach that combines the principles of reinforcement learning with the power of human feedback. The goal of RLHF is to learn a policy that can solve a task by interacting with the environment and receiving feedback from a human teacher.\\n\\nIn traditional reinforcement learning, an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. However, in many real-world scenarios, it can be difficult to design an environment that provides clear and accurate rewards, or to define a reward function that aligns with the desired outcome.\\n\\nRLHF addresses these challenges by incorporating human feedback into the learning process. The agent observes the environment and takes actions, and the human teacher provides feedback in the form of rewards or corrections. The agent uses this feedback to update its policy and improve its performance.\\n\\nRLHF has many potential applications, including robotics, game playing, and recommendation systems. For example, an RLHF agent could be trained to play a game like chess or Go, using human feedback to improve its moves and increase its chances of winning. Similarly, an RLHF agent could be used to recommend products or services to users, based on their feedback and preferences.\\n\\nOne advantage of RLHF is that it can learn from a small number of interactions with the environment, making it more efficient than traditional reinforcement learning methods. Additionally, RLHF can handle complex tasks that require a deep understanding of the environment, such as understanding natural language or recognizing images.\\n\\nHowever, RLHF also has some challenges that must be addressed. One of the main challenges is dealing with noisy or inconsistent feedback from the human teacher. If the feedback is not accurate or reliable, the agent may not be able to learn an effective policy. Another challenge is scaling RLHF to complex tasks or large environments, where the number of possible actions or states can be very large.\\n\\nOverall, RLHF is a promising approach to reinforcement learning that combines the power of human feedback with the flexibility of machine learning. With careful design and appropriate use, RLHF can help to solve complex tasks and improve the performance of machine learning systems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Continue\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YQgcmm1W75L8",
        "outputId": "db1a5d17-2778-4a60-b68f-af2b4241ce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Can you tell me about the olympics\n",
            "AI:   Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\n",
            "\n",
            "How may I assist you further?\n",
            "Human: What have we talked about in this Chat?\n",
            "AI:   Of course! Based on the chat history you provided, here are the topics we discussed:\n",
            "\n",
            "1. Introduction: The assistant greeted Sam and asked how they may assist them.\n",
            "2. Personal information: Sam asked the assistant to tell them about themselves, and the assistant provided some basic information about being an AI assistant.\n",
            "3. Days of the week: Sam asked the assistant what day of the week it was, and the assistant replied that it was Friday, which is the 5th day of the week.\n",
            "4. Name: Sam asked the assistant to tell them their name, and the assistant politely declined and asked if there was anything else they could help with.\n",
            "5. Olympics: Sam asked the assistant to tell them about the Olympics, and the assistant provided some general information about the Olympics, including that they are a series of international sporting events that take place every four years and are organized by the International Olympic Committee (IOC).\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions.\n",
            "Human: What is the Retrieval Augmetation Generation?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. The Retrieval Augmentation Generation (RAG) is a term used in the field of natural language processing (NLP) to describe a type of language model that is trained to generate text that is similar to a given prompt or input.\n",
            "\n",
            "RAG models are designed to improve the performance of language models by generating new text that is similar to the training data, but not necessarily identical to it. This can help the model to generate more diverse and creative responses to given prompts, and can also help to reduce the risk of overfitting to a particular dataset.\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions.\n",
            "Human: Explain RLHF in 300 words\n",
            "AI:   Of course! Reinforcement Learning with Human Feedback (RLHF) is a machine learning approach that combines the principles of reinforcement learning with the power of human feedback. The goal of RLHF is to learn a policy that can solve a task by interacting with the environment and receiving feedback from a human teacher.\n",
            "\n",
            "In traditional reinforcement learning, an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. However, in many real-world scenarios, it can be difficult to design an environment that provides clear and accurate rewards, or to define a reward function that aligns with the desired outcome.\n",
            "\n",
            "RLHF addresses these challenges by incorporating human feedback into the learning process. The agent observes the environment and takes actions, and the human teacher provides feedback in the form of rewards or corrections. The agent uses this feedback to update its policy and improve its performance.\n",
            "\n",
            "RLHF has many potential applications, including robotics, game playing, and recommendation systems. For example, an RLHF agent could be trained to play a game like chess or Go, using human feedback to improve its moves and increase its chances of winning. Similarly, an RLHF agent could be used to recommend products or services to users, based on their feedback and preferences.\n",
            "\n",
            "One advantage of RLHF is that it can learn from a small number of interactions with the environment, making it more efficient than traditional reinforcement learning methods. Additionally, RLHF can handle complex tasks that require a deep understanding of the environment, such as understanding natural language or recognizing images.\n",
            "\n",
            "However, RLHF also has some challenges that must be addressed. One of the main challenges is dealing with noisy or inconsistent feedback from the human teacher. If the feedback is not accurate or reliable, the agent may not be able to learn an effective policy. Another challenge is scaling RLHF to complex tasks or large environments, where the number of possible actions or states can be very large.\n",
            "\n",
            "Overall, RLHF is a promising approach to reinforcement learning that combines the power of human feedback with the flexibility of machine learning. With careful design and appropriate use, RLHF can help to solve complex tasks and improve the performance of machine learning systems. \n",
            "\n",
            "User: Continue[/INST]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Of course! I'd be happy to continue assisting you. What would you like to know or discuss today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Translate RLHF into Vietnamese\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yIntS8oL8pCz",
        "outputId": "ca7ff502-9ce3-4edd-b61f-fa6da9602df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:   Hello Sam! How may I assist you today?\n",
            "Human: Can you tell me about yourself.\n",
            "AI:   Of course, I'd be happy to help! As an AI assistant, I'm here to assist you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Today is Friday. What number day of the week is that?\n",
            "AI:   AI: Ah, a great question! Friday is the 5th day of the week.\n",
            "Human: What is my name?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. Can you tell me more about what you're looking for?\n",
            "Human: Can you tell me about the olympics\n",
            "AI:   Of course! The Olympics are a series of international sporting events that take place every four years, where athletes from around the world compete in various sports such as track and field, swimming, gymnastics, and more. The Olympics are organized by the International Olympic Committee (IOC) and are considered to be one of the most prestigious and widely viewed sporting events in the world.\n",
            "\n",
            "How may I assist you further?\n",
            "Human: What have we talked about in this Chat?\n",
            "AI:   Of course! Based on the chat history you provided, here are the topics we discussed:\n",
            "\n",
            "1. Introduction: The assistant greeted Sam and asked how they may assist them.\n",
            "2. Personal information: Sam asked the assistant to tell them about themselves, and the assistant provided some basic information about being an AI assistant.\n",
            "3. Days of the week: Sam asked the assistant what day of the week it was, and the assistant replied that it was Friday, which is the 5th day of the week.\n",
            "4. Name: Sam asked the assistant to tell them their name, and the assistant politely declined and asked if there was anything else they could help with.\n",
            "5. Olympics: Sam asked the assistant to tell them about the Olympics, and the assistant provided some general information about the Olympics, including that they are a series of international sporting events that take place every four years and are organized by the International Olympic Committee (IOC).\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions.\n",
            "Human: What is the Retrieval Augmetation Generation?\n",
            "AI:   Hello! I'm here to help you with any questions or tasks you may have. The Retrieval Augmentation Generation (RAG) is a term used in the field of natural language processing (NLP) to describe a type of language model that is trained to generate text that is similar to a given prompt or input.\n",
            "\n",
            "RAG models are designed to improve the performance of language models by generating new text that is similar to the training data, but not necessarily identical to it. This can help the model to generate more diverse and creative responses to given prompts, and can also help to reduce the risk of overfitting to a particular dataset.\n",
            "\n",
            "I hope that helps! Let me know if you have any other questions.\n",
            "Human: Explain RLHF in 300 words\n",
            "AI:   Of course! Reinforcement Learning with Human Feedback (RLHF) is a machine learning approach that combines the principles of reinforcement learning with the power of human feedback. The goal of RLHF is to learn a policy that can solve a task by interacting with the environment and receiving feedback from a human teacher.\n",
            "\n",
            "In traditional reinforcement learning, an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. However, in many real-world scenarios, it can be difficult to design an environment that provides clear and accurate rewards, or to define a reward function that aligns with the desired outcome.\n",
            "\n",
            "RLHF addresses these challenges by incorporating human feedback into the learning process. The agent observes the environment and takes actions, and the human teacher provides feedback in the form of rewards or corrections. The agent uses this feedback to update its policy and improve its performance.\n",
            "\n",
            "RLHF has many potential applications, including robotics, game playing, and recommendation systems. For example, an RLHF agent could be trained to play a game like chess or Go, using human feedback to improve its moves and increase its chances of winning. Similarly, an RLHF agent could be used to recommend products or services to users, based on their feedback and preferences.\n",
            "\n",
            "One advantage of RLHF is that it can learn from a small number of interactions with the environment, making it more efficient than traditional reinforcement learning methods. Additionally, RLHF can handle complex tasks that require a deep understanding of the environment, such as understanding natural language or recognizing images.\n",
            "\n",
            "However, RLHF also has some challenges that must be addressed. One of the main challenges is dealing with noisy or inconsistent feedback from the human teacher. If the feedback is not accurate or reliable, the agent may not be able to learn an effective policy. Another challenge is scaling RLHF to complex tasks or large environments, where the number of possible actions or states can be very large.\n",
            "\n",
            "Overall, RLHF is a promising approach to reinforcement learning that combines the power of human feedback with the flexibility of machine learning. With careful design and appropriate use, RLHF can help to solve complex tasks and improve the performance of machine learning systems.\n",
            "Human: Continue\n",
            "AI:   Of course! I'd be happy to continue assisting you. What would you like to know or discuss today? \n",
            "\n",
            "User: Translate RLHF into Vietnamese[/INST]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Hello! I\\'m here to help you with any questions or tasks you may have. Based on your request, I will translate \"Reinforcement Learning with Human Feedback\" (RLHF) into Vietnamese for you.\\n\\nTạm dĩ: Đào tạo học hỏi với nhận thức của con người (RLHF) là một phương pháp học hỏi của máy tính dựa trên đào tạo học hỏi của con người. Ngoài ra, RLHF cũng có thể được sử dụng để giải quyết các vấn đề phức tạp trong nhiều lĩnh vực, như robotics, trò chơi, và hệ thống đề xuất.\\n\\nI hope that helps! Let me know if you have any other questions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5y0KS5u9J70"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af6f1e22601e4a68b1a631d3d19f24d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eadcd287fe554bc897ece84471179ce8",
              "IPY_MODEL_b613ddcd4cbb4f18b1fb3b743d082fc9",
              "IPY_MODEL_5fce373fbe5f429ea6f443851bbead0c",
              "IPY_MODEL_c4610ae883b345869960eec412d7a36a"
            ],
            "layout": "IPY_MODEL_fc7c1c01f4df4809810d48ac6ebdeb7c"
          }
        },
        "8a351400706d4da4a7bf5465c902eb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de951e9d5c7d4c04b34ec25a1c51b225",
            "placeholder": "​",
            "style": "IPY_MODEL_5265e9a9f2094cb7b56a74cfc91003c0",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "7a6f269ee05a494f8ea9e2562586ec11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5f104238bcf448b299913272e524956c",
            "placeholder": "​",
            "style": "IPY_MODEL_681817af6438438f8882afebc5ba0d0f",
            "value": ""
          }
        },
        "22ab218d9c294411bd2f118ae8e12eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b7d18a7731904372acee05dfab680094",
            "style": "IPY_MODEL_680715e056614a588c6dd9496261ad30",
            "value": true
          }
        },
        "84a99924b765437899c7cdf9c0c9ddc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_565eac4c4a514d2c82e620b0868a337f",
            "style": "IPY_MODEL_5a913a833b954a2898bbadb11324e75d",
            "tooltip": ""
          }
        },
        "198732fb3afa428fa4f481dd8fca0f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7c20ff1514e4da0ad29dfda190d2763",
            "placeholder": "​",
            "style": "IPY_MODEL_d2f656b98c4a4ff9957a0f2d11fe4dbd",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "fc7c1c01f4df4809810d48ac6ebdeb7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "de951e9d5c7d4c04b34ec25a1c51b225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5265e9a9f2094cb7b56a74cfc91003c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f104238bcf448b299913272e524956c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681817af6438438f8882afebc5ba0d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7d18a7731904372acee05dfab680094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680715e056614a588c6dd9496261ad30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "565eac4c4a514d2c82e620b0868a337f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a913a833b954a2898bbadb11324e75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e7c20ff1514e4da0ad29dfda190d2763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f656b98c4a4ff9957a0f2d11fe4dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d54b469234f14ab59722298922185ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7846cfb18da54bb6a944f41f8fe97e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_c96828febb8149aea0510948c6c62085",
            "value": "Connecting..."
          }
        },
        "7846cfb18da54bb6a944f41f8fe97e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c96828febb8149aea0510948c6c62085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eadcd287fe554bc897ece84471179ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eaa651e4189445eb6ed30147f12e33b",
            "placeholder": "​",
            "style": "IPY_MODEL_92b6adec33f946ab9c90bb460b8f1146",
            "value": "Token is valid (permission: read)."
          }
        },
        "b613ddcd4cbb4f18b1fb3b743d082fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_202c474b79c44a229cf2722ee3d46de7",
            "placeholder": "​",
            "style": "IPY_MODEL_d0a3a2ecb5d944a7b771e15042dab818",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "5fce373fbe5f429ea6f443851bbead0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19360207a71d4254981d6f826c8f6e95",
            "placeholder": "​",
            "style": "IPY_MODEL_33685612875f4bf1ae2688cdbfb93cd3",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "c4610ae883b345869960eec412d7a36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c3cccdb21c423e84094ea443eb6c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_0ff24ed5b6d04c30b512a7fdab3fa9c2",
            "value": "Login successful"
          }
        },
        "7eaa651e4189445eb6ed30147f12e33b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92b6adec33f946ab9c90bb460b8f1146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "202c474b79c44a229cf2722ee3d46de7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a3a2ecb5d944a7b771e15042dab818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19360207a71d4254981d6f826c8f6e95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33685612875f4bf1ae2688cdbfb93cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15c3cccdb21c423e84094ea443eb6c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff24ed5b6d04c30b512a7fdab3fa9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b54ff2930747d69f2185e404e147cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_836405390036405db91880afd10b32a1",
              "IPY_MODEL_16e1e56c8ed24a2c880bd08aef948420",
              "IPY_MODEL_97deadf494454f0c8c7c246d79320036"
            ],
            "layout": "IPY_MODEL_72ea6d33efa349d8902e7abc1e80313d"
          }
        },
        "836405390036405db91880afd10b32a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ec9fd7961d34476a70beb35579d6d94",
            "placeholder": "​",
            "style": "IPY_MODEL_0f52f35bdbec4ee4b22d72b84a858b20",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "16e1e56c8ed24a2c880bd08aef948420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73d1782f8d143dc9c8fdb2d8d259fea",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51c5e7e09c894d6eadefb901a9f73487",
            "value": 2
          }
        },
        "97deadf494454f0c8c7c246d79320036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494fb611c431435cb0d0c3fdaaaca159",
            "placeholder": "​",
            "style": "IPY_MODEL_bf3e7407fd654fc39b2fc6ba5d8f754a",
            "value": " 2/2 [00:54&lt;00:00, 24.98s/it]"
          }
        },
        "72ea6d33efa349d8902e7abc1e80313d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec9fd7961d34476a70beb35579d6d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f52f35bdbec4ee4b22d72b84a858b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d73d1782f8d143dc9c8fdb2d8d259fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c5e7e09c894d6eadefb901a9f73487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "494fb611c431435cb0d0c3fdaaaca159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf3e7407fd654fc39b2fc6ba5d8f754a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}